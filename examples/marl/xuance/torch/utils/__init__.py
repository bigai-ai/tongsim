from .distributions import (
    ActivatedDiagGaussianDistribution,
    CategoricalDistribution,
    DiagGaussianDistribution,
    Distribution,
    SymLogDistribution,
    kl_div,
    merge_distributions,
    split_distributions,
)
from .layers import (
    ModuleType,
    Moments,
    cnn_block,
    gru_block,
    lstm_block,
    mlp_block,
    nn,
    pooling_block,
    torch,
)
from .operations import (
    assign_from_flat_grads,
    assign_from_flat_params,
    compute_lambda_values,
    compute_stochastic_state,
    dotdict,
    get_flat_grad,
    get_flat_params,
    init_distributed_mode,
    init_weights,
    set_seed,
    sym_exp,
    sym_log,
    two_hot_decoder,
    two_hot_encoder,
    uniform_init_weights,
    update_linear_decay,
)
from .value_norm import ValueNorm

ActivationFunctions = {
    "relu": nn.ReLU,
    "leaky_relu": nn.LeakyReLU,
    "tanh": nn.Tanh,
    "sigmoid": nn.Sigmoid,
    "softmax": nn.Softmax,
    "softmax2d": nn.Softmax2d,
    "elu": nn.ELU,
}

NormalizeFunctions = {
    "LayerNorm": nn.LayerNorm,
    "GroupNorm": nn.GroupNorm,
    "BatchNorm": nn.BatchNorm1d,
    "BatchNorm2d": nn.BatchNorm2d,
    "InstanceNorm2d": nn.InstanceNorm2d,
}

InitializeFunctions = {
    "orthogonal": torch.nn.init.orthogonal_,
    "normal": torch.nn.init.normal_,
    "zeros": torch.nn.init.zeros_,
    "ones": torch.nn.init.ones_,
}


__all__ = [
    "ActivatedDiagGaussianDistribution",
    "CategoricalDistribution",
    "DiagGaussianDistribution",
    "Distribution",
    "SymLogDistribution",
    "kl_div",
    "merge_distributions",
    "split_distributions",
    "ModuleType",
    "Moments",
    "cnn_block",
    "gru_block",
    "lstm_block",
    "mlp_block",
    "nn",
    "pooling_block",
    "torch",
    "assign_from_flat_grads",
    "assign_from_flat_params",
    "compute_lambda_values",
    "compute_stochastic_state",
    "dotdict",
    "get_flat_grad",
    "get_flat_params",
    "init_distributed_mode",
    "init_weights",
    "set_seed",
    "sym_exp",
    "sym_log",
    "two_hot_decoder",
    "two_hot_encoder",
    "uniform_init_weights",
    "update_linear_decay",
    "ValueNorm",
    "ActivationFunctions",
    "NormalizeFunctions",
    "InitializeFunctions",
]
